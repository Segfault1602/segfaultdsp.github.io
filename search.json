[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Segfault DSP",
    "section": "",
    "text": "Welcome to my personal page!"
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "Segfault DSP",
    "section": "Projects",
    "text": "Projects\n\nlibdsp: Simple C++ audio dsp library.\n2D Mesh Sandbox: C++ application for simulating 2D meshes using digital waveguides.\nBowedVst Polyphonic bowed string VST using JUCE.\nLinnLED: Python scripts used to configure the LED on the Linnstrument to match microtonal scales.\nDaisy Patch for VCV Rack: Implementation of the Daisy Patch eurorack module in VCV Rack. Allows easy prototyping of firmware for the Daisy Patch in VCV Rack."
  },
  {
    "objectID": "index.html#articles",
    "href": "index.html#articles",
    "title": "Segfault DSP",
    "section": "Articles",
    "text": "Articles"
  },
  {
    "objectID": "posts/bowed_string/bow_table.html",
    "href": "posts/bowed_string/bow_table.html",
    "title": "BowedString Part 2: Bow Table",
    "section": "",
    "text": "Now that we have our working waveguide, we need something to excite it. This is done with the help of a bow table. While the linear bow table would be simple to implement, we would need to know the capture or break-away differential velocity value \\(v_\\Delta^c\\) and also how this value responds to varying force values. I could not find a simple way to find these values but luckily, the STK library has a bow table that we can reuse.\nLet’s take a look at the bow table as defined in the stk:\n\\[ BowTable(v_\\Delta^+) = \\min \\left \\{(|v_\\Delta^+|*Slope + 0.75)^{-4}, 1 \\right \\} \\]\nWhere \\(v_\\Delta^+\\) is the velocity of the bow minus the velocity of the string going into the bow and \\(Slope\\) is the parameter that controls the shape of the table and is related to the bow force. While the equation may seem daunting at first, we can easily plot it and immediately recognize a shape similar to the linear bow table as presented by figure 9.54 in Physical Audio Signal Processing by Julius O. Smith.\n\n\nShow the code\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MultipleLocator\n\nv_delta = np.linspace(-1, 1, 50)\nslope = 4\n\noutput = np.minimum(pow(abs(v_delta) * slope + 0.75, -4), 1)\n\nplt.figure(1)\nplt.plot(v_delta, output)\nplt.xlabel('$v_\\Delta^+$')\nplt.ylabel('Reflection Coefficient')\nplt.grid()\n\n\n&lt;&gt;:12: SyntaxWarning: invalid escape sequence '\\D'\n&lt;&gt;:12: SyntaxWarning: invalid escape sequence '\\D'\n/var/folders/cc/n9ncrjvd4wz80h41vvwnpyl80000gn/T/ipykernel_94326/2598389576.py:12: SyntaxWarning: invalid escape sequence '\\D'\n  plt.xlabel('$v_\\Delta^+$')\n\n\n\n\n\n\n\n\n\nWe can now observe how varying the force transforms the bow table.\n\n\nShow the code\nv_delta = np.linspace(-1, 1, 100)\n\nplt.figure(2)\nfor f in [1, 2.5, 5]:\n    output = np.minimum(pow(abs(v_delta) * f + 0.75, -4), 1)\n    plt.plot(v_delta, output, label=f'slope={f}')\nplt.legend()\nplt.xlabel('$v_\\\\Delta^+$')\nplt.ylabel('Reflection Coefficient')\nplt.grid()\n\n\n\n\n\n\n\n\n\nAs \\(Slope\\) goes up, the region of the table where the reflection coefficient is 1 gets smaller. This plateau represents the moment where the bow and the string are “sticking” together.\nWe now need to find what is the usable range for \\(Slope\\). Looking at the STK again, we can see where the slope is set:\nbowed.cpp\n    bowTable_.setSlope( 5.0 - (4.0 * normalizedValue) );\nWhere normalizedValue is a value between 0 and 1 representing the bow pressure. This effectively restricts the slope value between 1 and 5. In other words, as the bow pressure increases, the ‘sticking’ zone of the bow table gets larger. We can plot the bow table equation again, but this time with varying force values between 0 and 10 to understand why.\n\n\nShow the code\nv_delta = np.linspace(-1, 1, 50)\nf_bow = np.linspace(0, 10, 50)\n\nv_delta, f_bow = np.meshgrid(v_delta, f_bow)\noutput = np.minimum(pow(abs(v_delta) * f_bow + 0.75, -4), 1)\n\nfig, ax = plt.subplots()\nsurf = ax.pcolormesh(\n    f_bow,\n    v_delta,\n    output,\n    linewidth=0,\n    cmap='viridis',)\n\nfig.colorbar(surf, shrink=0.5, aspect=5, label='Reflection Coefficient')\n\nax.yaxis.set_major_formatter('{x:.02f}')\nax.xaxis.set_major_locator(MultipleLocator(1))\nax.set_xlabel('$Slope$')\nax.set_ylabel('$v_\\\\Delta^+$')\nax.dist = 10\n\n\n\n\n\n\n\n\n\nLooking at the graph, we can immediately see that once the slope gets higher than 5, the reflection coefficient returned by the table stays consistent and there’s limited value for a bow table to support a slope value higher than 5. On the other hand, with a slope value \\(&lt;1\\), the table becomes almost flat.",
    "crumbs": [
      "Posts",
      "Bowed String",
      "BowedString Part 2: Bow Table"
    ]
  },
  {
    "objectID": "posts/bowed_string/waveguide.html",
    "href": "posts/bowed_string/waveguide.html",
    "title": "BowedString Part 1: Digital Waveguide",
    "section": "",
    "text": "In this series, I will walk through the process of building a bowed string model using a digital waveguide. If you are not familiar with the technique, Julius O. Smith wrote a great book on the subject: Physical Audio Signal Processing. The particular model I will build can also be found in this book here.\n\n\n\nBowed string model by Julius O. Smith\n\n\nOne possible way to implement this model would be to implement it exactly as presented using 4 distinct delay lines. This would work quite well for a static model, but since we want to be able to change the delay lines’ lengths in real-time, we will quickly run into some issues.\nTo understand, let’s look at a basic delay line implementation. The canonical delay line usually consists of a circular buffer and two pointers: one for reading and one for writing. Changing the delay length is as simple as changing the read pointer’s position. This looks something like this:\n\n\n\nA delay line. The read pointer moves to the left as the delay length is decreased and to the right as the delay length is increased.\n\n\nWhen the length of a delay line is changed in real-time, it is called a variable delay line. It is also possible to read between two sample points by doing an interpolated read. This would make the delay line a fractional delay line. Being able to read in between samples is important as it will allow us to smoothly change the delay length in real-time.\nA simple waveguide can be implemented using two delay lines, one for the right traveling wave and one for the left traveling wave.\n\n\n\nA simple waveguide\n\n\nNotice how the delay lines are going in opposite directions. Now, if we were to change the waveguide length by reducing both delay line lengths this would look like this:\n\n\n\nReducing the waveguide length. The red parts indicate the samples that get discarded as the delay is decreased.\n\n\nIf the waveguide was modeling a string, we’ve now effectively removed a bit of the right traveling signal from the right end of the string and a bit of the left traveling signal from the left end of the string. This is physically impossible and can introduce discontinuities in the signal. Instead, what we want to achieve is something like this:\n\n\n\nA more realistic waveguide.\n\n\nNotice how the size of the waveguide does not change and instead, a reflection point is introduced at some point on the string, effectively splitting the waveguide in two. This is attempting to simulate a finger pressing on the string.\nOn top of being able to read at an arbitrary fractional point on the delay line, we will also need to be able to write into the delay line at a fractional point. To do this, we can use an algorithm presented in Vesa Välimäki’s paper Discrete-Time Modeling of Acoustic Tubes Using Fractional Delay Filters. While Välimäki is describing a way to implement a scattering junction, we can use the same system to implement our reflection point by changing some of the coefficients. Instead of using an allpass filter to interpolate the signal I have opted to use a simple linear interpolator. Since we want to be able to vary the position of the junction in real-time, we would need to perform extra work to avoid the transient that would be introduced by using an allpass interpolation. A method to eliminate these transients is presented in this paper by Vesa Välimäki, Timo I. Laakso and Jonathan MacKenzie. A C++ implementation can be found in libdsp.\n\n\n\nBlock diagram of the scattering junction by Vesa Välimäki\n\n\n\n\n\nBlock diagram of the waveguide gate. The allpass interpolators were changed to linear interpolators and the coefficients were changed to conserve energy through the system.\n\n\nThe resulting system is what we will now call a waveguide gate. The coefficient ‘r’ is what I will call the gate coefficient. With a value of 1, the gate is fully closed and all the energy is reflected. With a value of 0, the gate is fully open and all the energy is transmitted. With a value between 0 and 1, the gate is partially open and some of the energy is reflected and some is transmitted.\nHere it is in action when the gate coefficient is set to 1:\n\n\n\nWaveguide gate with coefficient 1. The top graph shows the left and right traveling wave separetly and the bottom graph shows the sum of the two waves.\n\n\nBy setting the gate coefficient to &lt; 1, we can simulate a light press on the string which can result in harmonics based on the position of the gate:\n\n\n\nWaveguide gate with coefficient 0.5 at the midpoint of the string. The top graph shows the left and right traveling wave separetly and the bottom graph shows the sum of the two waves.\n\n\n\n\n\nWaveguide gate with coefficient 0.5 at the 1/4 of the string. The top graph shows the left and right traveling wave separetly and the bottom graph shows the sum of the two waves.\n\n\nHere are some audio examples of the gate in action using a waveguide with 200 samples of delay and a gate positioned in the middle of the string. A triangle pluck is used to excite the string.\nGate coefficient = 0 (open string):\n\n\n\n\n\nGate coefficient = 1 (full press):\n\n\n\n\n\nGate coefficient = 0.005 (The fundamental quickly morphs into the first harmonic):",
    "crumbs": [
      "Posts",
      "Bowed String",
      "BowedString Part 1: Digital Waveguide"
    ]
  },
  {
    "objectID": "posts/2d_mesh_sandbox/index.html",
    "href": "posts/2d_mesh_sandbox/index.html",
    "title": "2D Mesh Sandbox",
    "section": "",
    "text": "In this paper, I will present a sandbox application made to experiment with physical model of circular membranes using 2D digital waveguides.\nWhile my goal for this project was to model the multiple components of a drum, it became clear that simply modeling a circular membrane that sounds like a drum is already a complex task. This is why I decided to focus on this part of the project for now. The sandbox application I am presenting here should make it easier to experiment with all of the different aspects present in the physical model of a circular membrane implemented using 2D digital waveguides.\n\n\n\nThe first step in modeling a circular membrane, or any 2D object for that matter, is to solve the 2D wave equation:\n\\[\\begin{equation*}\nu_{tt} = c^2 \\Delta u\n\\end{equation*}\\]\n\n\n\n\n\n\n\nFigure 1: Rectilinear mesh\n\n\n\n\n\n\n\n\n\nFigure 2: Triangular mesh\n\n\n\n\nAs demonstrated by Van Duyne and Smith [1], the 2D digital waveguide mesh can be used to accurately model the wave equation. As opposed to its 1D counterpart, there is more than one way to implement a 2D digital waveguide mesh. Maybe the most common way is to use the rectilinear mesh, composed of a grid of interconnected 1D waveguides.\nWhile probably the simplest mesh to implement, the rectilinear mesh suffers from quite a bit of dispersion error which can result in mistuning of the higher modes of the membrane[1].\nAnother popular mesh configuration is the triangular mesh. Composed of a series of 6-port junctions, the triangular mesh is slightly more complex to implement but has the advantage of having a much lower dispersion error than the rectilinear mesh [2].\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Dispersion error of the rectilinear mesh[2]\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Dispersion error of the triangular mesh[2]\n\n\n\n\n\n\nThe rectilinear mesh is implemented by the RectilinearMesh class and the triangular mesh is implemented by the TriMesh class.\n\n\nThe next step is to model the circular boundary. By using the rectilinear or triangular mesh alone, the best we can do is to approximate the circular boundary given the naturally jagged edge of the meshes. To get a more accurate representation of the circular boundary, Laird proposes the use of special 1D waveguides called ‘rimguides’ [3]. For each junction on the boundary of the membrane, a rimguide is connected to that junction. The length of each rimguide is set such that a wave traveling from one side of the membrane to the other will take the same amount of time as it would in an ideal membrane. Figure 5 shows a triangular mesh with rimguides (green nodes and dotted lines) connected to each boundary node (in blue).\n\n\n\n\n\n\nFigure 5: Triangular mesh with rimguides\n\n\n\nThe simplest rimguide will have a minimal delay of 1.5 samples. A 1 sample delay is incurred due to the nature of the waveguide mesh itself and the 0.5 sample delay is due to the fact that allpass interpolation is used inside the rimguide and that type of interpolation works best for delays between 0.5 and 1.5 samples.\nGiven this information, the radius of the mesh will need to be adjusted to account for the delay of the rimguides. This new radius (red circle in Figure 5) is given by the following formula:\n\\[\\begin{equation*}\nr_{new} = r_{old} - \\frac{\\tau_{min}}{2} * s_d\\quad,\n\\end{equation*}\\]\nwhere \\(r_{old}\\) is the radius of the mesh without rimguides, \\(\\tau_{min} = 1.5\\) is the minimal delay of the rimguides and \\(s_d\\) is the spatial distance between two nodes of the mesh. The division by 2 is due to the fact that the rimguide models the wave traveling from the boundary node to the rim and back.\nThe spatial distance \\(s_d\\) is given by the following formula: \\[\\begin{equation*}\ns_d = \\sqrt{2} \\frac{c}{f_s}\\quad,\n\\end{equation*}\\]\nwhere \\(c\\) is the speed of sound in the membrane and \\(f_s\\) is the sampling frequency. The \\(\\sqrt{2}\\) factor is due to the nominal wave speed in a 2D mesh [[1]][2].\nThe speed of sound \\(c\\) in the membrane can be found with the following formula[3]: \\[\\begin{equation*}\nc = \\sqrt{\\frac{T}{\\sigma}}\\quad,\n\\end{equation*}\\]\nwhere \\(T\\) is the tension of the membrane in \\(\\frac{N}{m}\\) and \\(\\sigma\\) is the density of the membrane in \\(\\frac{kg}{m^2}\\).\n\n\n\nSo far the model is lossless. To account for viscous friction, we can add a first order IIR lowpass filter to our rimguides. The coefficient for the lowpass filter is given by the following formula [3]:\n\\[\\begin{equation*}\n\\alpha =\\frac{1 - G(\\omega)\\left(G(\\omega)\\cos(\\omega) + \\sqrt{G(\\omega)^2(\\cos^2\\omega-1)+2-2\\cos\\omega}\\right)}{G(\\omega)^2 - 1}\\quad,\n\\end{equation*}\\]\nwhere \\(G(\\omega)\\) is the gain of the filter at frequency \\(\\omega\\). The gain of the filter is given by the following formulas:\n\\[\\begin{align*}\nG(\\omega) &= 10 ^ \\frac{L_B(\\omega)}{20}\\\\\nL_B(\\omega) &= L_\\omega \\frac{2r}{c}\\quad,\n\\end{align*}\\]\nwhere \\(L_\\omega\\) is the desired decay rate at frequency \\(\\omega\\), \\(r\\) is the radius of the membrane and \\(c\\) is the speed of sound in the membrane.\nSince this lowpass filter introduces an additional delay into our rimguide, the radius of the mesh will need to be adjusted again to account for this delay. The delay incurred by the lowpass filter is given by the following formula:\n\\[\\begin{equation*}\n\\tau_{lp} = \\frac{\\arctan(\\frac{-\\alpha sin\\omega}{1 + \\alpha cos\\omega})}{\\omega}\\quad,\n\\end{equation*}\\]\nwhere \\(\\omega\\) is the cutoff frequency of the lowpass filter in radians per second.\n\n\n\nHere’s what the current model sounds like when excited with a simple raised cosine impulse:\n\n\n\n\n\n\n32 cm radius, center excitation Your browser does not support the audio tag. \n\n\n32 cm radius, off-center excitation Your browser does not support the audio tag. \n\n\n\n\n\n\n\nOnce I had a working model of a circular membrane, I wanted to experiment with different aspect of the model.\n\n\nThe first thing I wanted to experiment with was the minimum delay of the rimguides. In theory, we want to minimize the delay of the rimguides to minimize the error introduced by the rimguides. However, a longer rimguide delay will also reduce the number of junctions needed to model the membrane, so I was curious to see exactly how much of a difference the delay of the rimguides would make.\n\n\n\n\n\n\nSpectrum of a 32 cm radius membrane with 1.5 sample minimum delay rimguides (red) and 3 sample minimum delay rimguides (blue)\n\n\nSpectrum of a 32 cm radius membrane with 1.5 sample minimum delay rimguides (red) and 9 sample minimum delay rimguides (blue)\n\n\n\n\n\n\n\n\n\n\n\nWhile the difference between 1.5 sample minimum delay rimguides and 3 sample minimum delay rimguides is quite small, we can see a noticeable pitch shift with 9 sample minimum delay rimguides. While the mesh with 1.5 sample minimum delay rimguides requires 1519 junctions, this number is reduced to 1015 junctions with a 9 sample minimum delay rimguides. Most importantly, the characteristic modes of the membrane are still present with 9 sample minimum delay rimguides.\n\n\n\nAnothe reason why I wanted to experiment with the rimguides delay is that I wanted to see if I could use time-varying delay to model the effect of the membrane stretching or shrinking. Time-varying delaylines are common in a lot of digital audio effects (chorus, flanger, etc.) and are also often used in 1D digital waveguides to model vibrato or pitch bending.\nSeveral modulation techniques were tested to modulate the delay of the rimguides. For all of these techniques, a sine wave was used as the modulation signal.\n\nSync\n\nEach rimguide is modulated by the same sine wave (same phase, frequency, and amplitude). This creates a vibrato effect.\n\n\nYour browser does not support the audio tag.\n\nPhase Offset\n\nEach rimguide is modulated by the same sine wave offset by a certain phase amount in relation to its neighbor. The effect on the sound is much more subtle than the sync modulation.\n\n\nYour browser does not support the audio tag.\n\nRandom Frequency\n\nEach rimguide is modulated by a sine wave with a random frequency. The random frequency is controlled by secondary parameters that dictate how much the frequency can vary from the base frequency.\n\n\nYour browser does not support the audio tag.\n\nRandom Frequency and Amplitude\n\nEach rimguide is modulated by a sine wave with a random frequency and amplitude. The random frequency and amplitude are controlled by secondary parameters that dictate how much the frequency and amplitude can vary from the base frequency and amplitude.\n\n\nYour browser does not support the audio tag.\n\n\n\nA well known nonlinear effect of a drum membrane is the pitch glide downward that occurs when the membrane is struck hard[4]. By adding a simple envelope follower inside the rimguides, I was able to modulate the delay of the rimguides based on the amplitude of the pressure wave at the boundary. The envelope follower is implemented as a simple two poles lowpass filter.\nYour browser does not support the audio tag.\n\n\n\nBy default, a clamped boundary condition is used for the membrane. This means that the displacement of the membrane at the boundary is fixed and a phase inversion is applied to the reflected wave. I added the ability to switch between clamped and free boundary conditions to see how it would affect the sound of the membrane.\n\n\n\n\n\n\nClamped boundary condition Your browser does not support the audio tag.\n\n\nFree boundary condition Your browser does not support the audio tag.\n\n\n\nIt is also possible to ‘clamp’ the center of the membrane. The center junction is removed and rimguides are added to the center of the membrane. This was partly inspired by how cymbals are usually mounted in the center while the edge of the cymbal is free to vibrate.\nYour browser does not support the audio tag.\n\n\n\nAlso implemented is a simple square law nonlinearity, as presented by Pierce and Van Duyne [5].\n\n\n\n\n\n\nFigure 6: Delay loop string model with square law nonlinearity\n\n\n\nThe nonlinearity was added to the rimguides. Apart from severely damping the sound, I was not able to get any interesting sound out of this system.\n\n\n\nAlso from Pierce and Van Duyne, the passive nonlinear allpass filter is a filter where the coefficient alternate between two values, depending on the sign of the internal state of the filter [5]. The idea is to model a string terminated by a string which coefficient changes depending on if the spring is compressed or extended.\n\n\n\n\n\n\nFigure 7: String terminated with passive nonlinear double spring\n\n\n\nWhile it is difficult to tune correctly, it is possible to get some interesting sounds out of this system.\n\n\n\n\n\n\nBase sound Your browser does not support the audio tag.\n\n\nWith passive nonlinear filter Your browser does not support the audio tag.\n\n\n\n\n\n\nFinally, each rimguides can support an abitrary amount of diffusion filters. The filters are implemented as a series of cascaded allpass filters.\n\n\n\n\n\n\nBase sound Your browser does not support the audio tag.\n\n\nWith 4 additional diffusion filters Your browser does not support the audio tag.\n\n\n\nHere’s a block diagram of the rimguide with optional diffusion filters:\n\n\n\n\n\n2D Mesh Sandbox is a C++ application that allows you to experiment with the physical model of a circular membrane using 2D digital waveguides. The application uses Dear ImGui and ImPlot for the user interface, RtAudio for audio output and libsndfile for audio file output and input. The source code is available on GitHub\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTriangular mesh \n\n\nRectilinear mesh",
    "crumbs": [
      "Posts",
      "2D Mesh Sandbox"
    ]
  },
  {
    "objectID": "posts/2d_mesh_sandbox/index.html#introduction",
    "href": "posts/2d_mesh_sandbox/index.html#introduction",
    "title": "2D Mesh Sandbox",
    "section": "",
    "text": "In this paper, I will present a sandbox application made to experiment with physical model of circular membranes using 2D digital waveguides.\nWhile my goal for this project was to model the multiple components of a drum, it became clear that simply modeling a circular membrane that sounds like a drum is already a complex task. This is why I decided to focus on this part of the project for now. The sandbox application I am presenting here should make it easier to experiment with all of the different aspects present in the physical model of a circular membrane implemented using 2D digital waveguides.",
    "crumbs": [
      "Posts",
      "2D Mesh Sandbox"
    ]
  },
  {
    "objectID": "posts/2d_mesh_sandbox/index.html#physical-modeling-of-a-circular-membrane",
    "href": "posts/2d_mesh_sandbox/index.html#physical-modeling-of-a-circular-membrane",
    "title": "2D Mesh Sandbox",
    "section": "",
    "text": "The first step in modeling a circular membrane, or any 2D object for that matter, is to solve the 2D wave equation:\n\\[\\begin{equation*}\nu_{tt} = c^2 \\Delta u\n\\end{equation*}\\]\n\n\n\n\n\n\n\nFigure 1: Rectilinear mesh\n\n\n\n\n\n\n\n\n\nFigure 2: Triangular mesh\n\n\n\n\nAs demonstrated by Van Duyne and Smith [1], the 2D digital waveguide mesh can be used to accurately model the wave equation. As opposed to its 1D counterpart, there is more than one way to implement a 2D digital waveguide mesh. Maybe the most common way is to use the rectilinear mesh, composed of a grid of interconnected 1D waveguides.\nWhile probably the simplest mesh to implement, the rectilinear mesh suffers from quite a bit of dispersion error which can result in mistuning of the higher modes of the membrane[1].\nAnother popular mesh configuration is the triangular mesh. Composed of a series of 6-port junctions, the triangular mesh is slightly more complex to implement but has the advantage of having a much lower dispersion error than the rectilinear mesh [2].\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Dispersion error of the rectilinear mesh[2]\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Dispersion error of the triangular mesh[2]\n\n\n\n\n\n\nThe rectilinear mesh is implemented by the RectilinearMesh class and the triangular mesh is implemented by the TriMesh class.\n\n\nThe next step is to model the circular boundary. By using the rectilinear or triangular mesh alone, the best we can do is to approximate the circular boundary given the naturally jagged edge of the meshes. To get a more accurate representation of the circular boundary, Laird proposes the use of special 1D waveguides called ‘rimguides’ [3]. For each junction on the boundary of the membrane, a rimguide is connected to that junction. The length of each rimguide is set such that a wave traveling from one side of the membrane to the other will take the same amount of time as it would in an ideal membrane. Figure 5 shows a triangular mesh with rimguides (green nodes and dotted lines) connected to each boundary node (in blue).\n\n\n\n\n\n\nFigure 5: Triangular mesh with rimguides\n\n\n\nThe simplest rimguide will have a minimal delay of 1.5 samples. A 1 sample delay is incurred due to the nature of the waveguide mesh itself and the 0.5 sample delay is due to the fact that allpass interpolation is used inside the rimguide and that type of interpolation works best for delays between 0.5 and 1.5 samples.\nGiven this information, the radius of the mesh will need to be adjusted to account for the delay of the rimguides. This new radius (red circle in Figure 5) is given by the following formula:\n\\[\\begin{equation*}\nr_{new} = r_{old} - \\frac{\\tau_{min}}{2} * s_d\\quad,\n\\end{equation*}\\]\nwhere \\(r_{old}\\) is the radius of the mesh without rimguides, \\(\\tau_{min} = 1.5\\) is the minimal delay of the rimguides and \\(s_d\\) is the spatial distance between two nodes of the mesh. The division by 2 is due to the fact that the rimguide models the wave traveling from the boundary node to the rim and back.\nThe spatial distance \\(s_d\\) is given by the following formula: \\[\\begin{equation*}\ns_d = \\sqrt{2} \\frac{c}{f_s}\\quad,\n\\end{equation*}\\]\nwhere \\(c\\) is the speed of sound in the membrane and \\(f_s\\) is the sampling frequency. The \\(\\sqrt{2}\\) factor is due to the nominal wave speed in a 2D mesh [[1]][2].\nThe speed of sound \\(c\\) in the membrane can be found with the following formula[3]: \\[\\begin{equation*}\nc = \\sqrt{\\frac{T}{\\sigma}}\\quad,\n\\end{equation*}\\]\nwhere \\(T\\) is the tension of the membrane in \\(\\frac{N}{m}\\) and \\(\\sigma\\) is the density of the membrane in \\(\\frac{kg}{m^2}\\).\n\n\n\nSo far the model is lossless. To account for viscous friction, we can add a first order IIR lowpass filter to our rimguides. The coefficient for the lowpass filter is given by the following formula [3]:\n\\[\\begin{equation*}\n\\alpha =\\frac{1 - G(\\omega)\\left(G(\\omega)\\cos(\\omega) + \\sqrt{G(\\omega)^2(\\cos^2\\omega-1)+2-2\\cos\\omega}\\right)}{G(\\omega)^2 - 1}\\quad,\n\\end{equation*}\\]\nwhere \\(G(\\omega)\\) is the gain of the filter at frequency \\(\\omega\\). The gain of the filter is given by the following formulas:\n\\[\\begin{align*}\nG(\\omega) &= 10 ^ \\frac{L_B(\\omega)}{20}\\\\\nL_B(\\omega) &= L_\\omega \\frac{2r}{c}\\quad,\n\\end{align*}\\]\nwhere \\(L_\\omega\\) is the desired decay rate at frequency \\(\\omega\\), \\(r\\) is the radius of the membrane and \\(c\\) is the speed of sound in the membrane.\nSince this lowpass filter introduces an additional delay into our rimguide, the radius of the mesh will need to be adjusted again to account for this delay. The delay incurred by the lowpass filter is given by the following formula:\n\\[\\begin{equation*}\n\\tau_{lp} = \\frac{\\arctan(\\frac{-\\alpha sin\\omega}{1 + \\alpha cos\\omega})}{\\omega}\\quad,\n\\end{equation*}\\]\nwhere \\(\\omega\\) is the cutoff frequency of the lowpass filter in radians per second.\n\n\n\nHere’s what the current model sounds like when excited with a simple raised cosine impulse:\n\n\n\n\n\n\n32 cm radius, center excitation Your browser does not support the audio tag. \n\n\n32 cm radius, off-center excitation Your browser does not support the audio tag.",
    "crumbs": [
      "Posts",
      "2D Mesh Sandbox"
    ]
  },
  {
    "objectID": "posts/2d_mesh_sandbox/index.html#experimentation",
    "href": "posts/2d_mesh_sandbox/index.html#experimentation",
    "title": "2D Mesh Sandbox",
    "section": "",
    "text": "Once I had a working model of a circular membrane, I wanted to experiment with different aspect of the model.\n\n\nThe first thing I wanted to experiment with was the minimum delay of the rimguides. In theory, we want to minimize the delay of the rimguides to minimize the error introduced by the rimguides. However, a longer rimguide delay will also reduce the number of junctions needed to model the membrane, so I was curious to see exactly how much of a difference the delay of the rimguides would make.\n\n\n\n\n\n\nSpectrum of a 32 cm radius membrane with 1.5 sample minimum delay rimguides (red) and 3 sample minimum delay rimguides (blue)\n\n\nSpectrum of a 32 cm radius membrane with 1.5 sample minimum delay rimguides (red) and 9 sample minimum delay rimguides (blue)\n\n\n\n\n\n\n\n\n\n\n\nWhile the difference between 1.5 sample minimum delay rimguides and 3 sample minimum delay rimguides is quite small, we can see a noticeable pitch shift with 9 sample minimum delay rimguides. While the mesh with 1.5 sample minimum delay rimguides requires 1519 junctions, this number is reduced to 1015 junctions with a 9 sample minimum delay rimguides. Most importantly, the characteristic modes of the membrane are still present with 9 sample minimum delay rimguides.\n\n\n\nAnothe reason why I wanted to experiment with the rimguides delay is that I wanted to see if I could use time-varying delay to model the effect of the membrane stretching or shrinking. Time-varying delaylines are common in a lot of digital audio effects (chorus, flanger, etc.) and are also often used in 1D digital waveguides to model vibrato or pitch bending.\nSeveral modulation techniques were tested to modulate the delay of the rimguides. For all of these techniques, a sine wave was used as the modulation signal.\n\nSync\n\nEach rimguide is modulated by the same sine wave (same phase, frequency, and amplitude). This creates a vibrato effect.\n\n\nYour browser does not support the audio tag.\n\nPhase Offset\n\nEach rimguide is modulated by the same sine wave offset by a certain phase amount in relation to its neighbor. The effect on the sound is much more subtle than the sync modulation.\n\n\nYour browser does not support the audio tag.\n\nRandom Frequency\n\nEach rimguide is modulated by a sine wave with a random frequency. The random frequency is controlled by secondary parameters that dictate how much the frequency can vary from the base frequency.\n\n\nYour browser does not support the audio tag.\n\nRandom Frequency and Amplitude\n\nEach rimguide is modulated by a sine wave with a random frequency and amplitude. The random frequency and amplitude are controlled by secondary parameters that dictate how much the frequency and amplitude can vary from the base frequency and amplitude.\n\n\nYour browser does not support the audio tag.\n\n\n\nA well known nonlinear effect of a drum membrane is the pitch glide downward that occurs when the membrane is struck hard[4]. By adding a simple envelope follower inside the rimguides, I was able to modulate the delay of the rimguides based on the amplitude of the pressure wave at the boundary. The envelope follower is implemented as a simple two poles lowpass filter.\nYour browser does not support the audio tag.\n\n\n\nBy default, a clamped boundary condition is used for the membrane. This means that the displacement of the membrane at the boundary is fixed and a phase inversion is applied to the reflected wave. I added the ability to switch between clamped and free boundary conditions to see how it would affect the sound of the membrane.\n\n\n\n\n\n\nClamped boundary condition Your browser does not support the audio tag.\n\n\nFree boundary condition Your browser does not support the audio tag.\n\n\n\nIt is also possible to ‘clamp’ the center of the membrane. The center junction is removed and rimguides are added to the center of the membrane. This was partly inspired by how cymbals are usually mounted in the center while the edge of the cymbal is free to vibrate.\nYour browser does not support the audio tag.\n\n\n\nAlso implemented is a simple square law nonlinearity, as presented by Pierce and Van Duyne [5].\n\n\n\n\n\n\nFigure 6: Delay loop string model with square law nonlinearity\n\n\n\nThe nonlinearity was added to the rimguides. Apart from severely damping the sound, I was not able to get any interesting sound out of this system.\n\n\n\nAlso from Pierce and Van Duyne, the passive nonlinear allpass filter is a filter where the coefficient alternate between two values, depending on the sign of the internal state of the filter [5]. The idea is to model a string terminated by a string which coefficient changes depending on if the spring is compressed or extended.\n\n\n\n\n\n\nFigure 7: String terminated with passive nonlinear double spring\n\n\n\nWhile it is difficult to tune correctly, it is possible to get some interesting sounds out of this system.\n\n\n\n\n\n\nBase sound Your browser does not support the audio tag.\n\n\nWith passive nonlinear filter Your browser does not support the audio tag.\n\n\n\n\n\n\nFinally, each rimguides can support an abitrary amount of diffusion filters. The filters are implemented as a series of cascaded allpass filters.\n\n\n\n\n\n\nBase sound Your browser does not support the audio tag.\n\n\nWith 4 additional diffusion filters Your browser does not support the audio tag.\n\n\n\nHere’s a block diagram of the rimguide with optional diffusion filters:",
    "crumbs": [
      "Posts",
      "2D Mesh Sandbox"
    ]
  },
  {
    "objectID": "posts/2d_mesh_sandbox/index.html#d-mesh-sandbox",
    "href": "posts/2d_mesh_sandbox/index.html#d-mesh-sandbox",
    "title": "2D Mesh Sandbox",
    "section": "",
    "text": "2D Mesh Sandbox is a C++ application that allows you to experiment with the physical model of a circular membrane using 2D digital waveguides. The application uses Dear ImGui and ImPlot for the user interface, RtAudio for audio output and libsndfile for audio file output and input. The source code is available on GitHub\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTriangular mesh \n\n\nRectilinear mesh",
    "crumbs": [
      "Posts",
      "2D Mesh Sandbox"
    ]
  },
  {
    "objectID": "posts/windowed_sinc/windowed_sinc.html",
    "href": "posts/windowed_sinc/windowed_sinc.html",
    "title": "Windowed sinc resampling",
    "section": "",
    "text": "While there is a lot of theoretical explanation of the sinc resampling algorithm on the internet, there is unfortunately not a lot of simple practical examples as the implementation are usually heavily optimized and designed to support real-time scenario. This article will try to provide a simple implementation of the algorithm based on the CCRMA description article. You can find a plain Python script containing the code used in this article here.\nFirst, let’s get our python imports out of the way.\n\n\nShow the code\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import sinc\nfrom scipy.signal.windows import kaiser\nfrom scipy import signal\nfrom math import ceil\n\n\nLet’s define a function that will build our sinc table for us. The sinc table will only contain the “right wing” of the symmetric FIR filter. This function will also return a table of differences between successive FIR samples which will be useful to speed up our linear interpolation later on.\n\ndef build_sinc_table(num_zeros, samples_per_crossing):\n\n    SINC_SIZE_ = num_zeros * samples_per_crossing\n    KAISER_BETA = 10\n\n    x = np.linspace(-num_zeros, num_zeros, SINC_SIZE_ * 2 + 1)\n    y = sinc(x)\n\n    window = kaiser(len(y), KAISER_BETA)\n\n    y = np.multiply(y, window)\n\n    h = y[SINC_SIZE_:]\n\n    h_diff = np.subtract(h[1:], h[:-1])\n    h_diff = np.append(h_diff, 0)\n\n    return h, h_diff\n\nAnd here’s what our table looks like:\n\n\nShow the code\nh, h_diff = build_sinc_table(num_zeros=13, samples_per_crossing=512)\nplt.figure(1)\nplt.plot(h)\nplt.grid()\n\n\n\n\n\n\n\n\n\nWe can now implement the main algorithm as described here.\n\ndef sinc_resample(x, ratio, h, h_diff, samples_per_crossing):\n    time_step = 1 / ratio\n    filter_scale = min(1, ratio)\n    filter_step = samples_per_crossing * filter_scale\n\n    output = np.ndarray(shape=ceil(len(x) * ratio))\n    out_idx = 0\n    t = 0\n    while t &lt; len(x):\n\n        acc = 0\n\n        integer_part = int(t)\n        fractional_part = t - integer_part\n\n        # Compute left wing\n        filter_offset = filter_step * fractional_part\n\n        left_coeff_count = int((len(h) - filter_offset) / filter_step)\n        left_coeff_count = min(integer_part, left_coeff_count) # avoid underflow access\n        for i in range(-left_coeff_count, 1):\n            filter_idx = filter_offset + filter_step * abs(i)\n            fraction = filter_idx - int(filter_idx)\n            filter_idx = int(filter_idx)\n\n            weight = h[filter_idx] + fraction * h_diff[filter_idx]\n            acc += x[integer_part + i] * weight\n\n        # compute right wing\n        fractional_part = 1 - fractional_part\n        filter_offset = filter_step * fractional_part\n\n        right_coeff_count = int((len(h) - filter_offset) / filter_step)\n        right_coeff_count = min(len(x) - integer_part - 1, right_coeff_count) # avoid overflow access\n        for i in range(0, right_coeff_count):\n            filter_idx = filter_offset + filter_step * i\n            fraction = filter_idx - int(filter_idx)\n            filter_idx = int(filter_idx)\n\n            weight = h[filter_idx] + fraction * h_diff[filter_idx]\n            acc += x[integer_part + 1 + i] * weight\n\n        if out_idx &lt; len(output):\n            output[out_idx] = acc\n            out_idx += 1\n        t += time_step\n\n    return output\n\nHere’s an example of the implementation at work where we will upsample a simple sine wave. The blue dotted line on the graph effectively shows what we would have obtained using a linear interpolation method instead.\n\n# Number of zeros crossing\nNZ = 13\n\n# Number of samples per zero crossing.\n# Higher sample count means better precision for our interpolation at the cost of more memory usage.\nSAMPLES_PER_CROSSING = 128\nh, h_diff = build_sinc_table(NZ, SAMPLES_PER_CROSSING)\n\nORIGINAL_FS = 100\nSIGNAL_FREQUENCY = 20\nTARGET_FS = 1000\n\ntime = np.linspace(0, 1, ORIGINAL_FS, endpoint=False)\nin_sine = np.sin(2 * np.pi * time * SIGNAL_FREQUENCY)\n\noutput = sinc_resample(in_sine, TARGET_FS / ORIGINAL_FS, h, h_diff, SAMPLES_PER_CROSSING)\nout_time = np.linspace(0, 1, TARGET_FS)\nplt.plot(out_time, output, 'g', label=\"Resampled output\")\nplt.plot(time, in_sine, 'b+:', label=\"Original signal\")\nplt.xlim(0.2, 0.4)\nplt.ylim(-1.1, 1.1)\nplt.grid()\nplt.legend(loc=\"upper right\")\nplt.show()\n\n\n\n\n\n\n\n\nWe are now ready to perform our benchmark test. Inspired by the Infinite Wave methodology, we will try to downsample a quadratic chirp signal from 96kHz to 44.1kHz.\n\nORIGINAL_FS = 96000\nCHIRP_LENGTH_SECONDS = 8\nEND_CHIRP_FREQUENCY = 44000\n\ntime = np.linspace(0, CHIRP_LENGTH_SECONDS, ORIGINAL_FS*CHIRP_LENGTH_SECONDS)\nin_chirp = signal.chirp(time, 0, CHIRP_LENGTH_SECONDS, END_CHIRP_FREQUENCY, 'quadratic') * 0.6\n\nWe will now resample that signal in 3 different ways. First, we’ll use our sinc_resample method with a sinc table containing 13 zero crossings and then again with a table containing 32 zero crossings. This should allow us to see how the number of zero crossing in our table affects the lowpass filtering of our implementation. Lastly, we will use numpy to resample the signal using linear interpolation so that we can compare our algorithm against a fast common resampling method.\n\nTARGET_FS = 44100\nRESAMPLING_RATIO = TARGET_FS / ORIGINAL_FS\n\nnz_1 = 13\nSAMPLES_PER_CROSSING = 128\nh, h_diff = build_sinc_table(nz_1, SAMPLES_PER_CROSSING)\n\nout_sinc_1 = sinc_resample(\n    in_chirp,\n    RESAMPLING_RATIO,\n    h,\n    h_diff,\n    SAMPLES_PER_CROSSING)\n\nnz_2 = 32\nSAMPLES_PER_CROSSING = 512\nh, h_diff = build_sinc_table(nz_2, SAMPLES_PER_CROSSING)\n\nout_sinc_2 = sinc_resample(\n    in_chirp,\n    RESAMPLING_RATIO,\n    h,\n    h_diff,\n    SAMPLES_PER_CROSSING)\n\nout_time = np.linspace(0, CHIRP_LENGTH_SECONDS, TARGET_FS*CHIRP_LENGTH_SECONDS)\nout_linear = np.interp(out_time, time, in_chirp)\n\ndef plot_spectrogram(title, w, fs, ax = None):\n    if ax is None:\n        fig, ax = plt.subplots()\n    plt.specgram(w, Fs=fs, mode='magnitude')\n    ax.set_title(title)\n    ax.set_xlabel('t (sec)')\n    ax.set_ylabel('Frequency (Hz)')\n    ax.set_ylim(0, ORIGINAL_FS/2)\n    ax.grid(True)\n\nfig = plt.figure(1)\nax1 = plt.subplot(221)\nplot_spectrogram(f\"Original {ORIGINAL_FS} Hz\", in_chirp, ORIGINAL_FS, ax1)\nax2 = plt.subplot(222)\nplot_spectrogram(f\"Resampled to {TARGET_FS} Hz, {nz_1} zeros\", out_sinc_1, TARGET_FS, ax2)\nax2 = plt.subplot(223)\nplot_spectrogram(f\"Resampled to {TARGET_FS} Hz, {nz_2} zeros\", out_sinc_2, TARGET_FS, ax2)\nax3 = plt.subplot(224)\nplot_spectrogram(f\"Resampled to {TARGET_FS} Hz (numpy.interp)\", out_linear, TARGET_FS, ax3)\nfig.tight_layout(pad=1.0)\nplt.show()\n\n/opt/miniconda3/lib/python3.12/site-packages/matplotlib/axes/_axes.py:8091: RuntimeWarning: divide by zero encountered in log10\n  Z = 20. * np.log10(spec)\n\n\n\n\n\n\n\n\n\nFirst, we can immediately notice that the resampled output lost all content above the Nyquist frequency. This explains why half the spectrogram is empty. The downward line(s) around the 6-second mark is the higher frequency content present in the original file that is now folding around Nyquist. This is called aliasing. We can also see how the resampling done with 32 zeros shows slightly less aliasing than the 13 zeros resampling.\nFinally, we can look at the impulse and frequency response of our resampler:\n\n\nShow the code\nORIGINAL_FS = 96000\nTARGET_FS = 44100\nRESAMPLING_RATIO = TARGET_FS / ORIGINAL_FS\n\nIMPULSE_LENGTH = 128\nimpulse = np.zeros(IMPULSE_LENGTH)\nimpulse[round(IMPULSE_LENGTH/2)] = 1\n\nnz_1 = 13\nSAMPLES_PER_CROSSING = 512\nh, h_diff = build_sinc_table(nz_1, SAMPLES_PER_CROSSING)\n\nout_imp_1 = sinc_resample(\n    impulse,\n    RESAMPLING_RATIO,\n    h,\n    h_diff,\n    SAMPLES_PER_CROSSING)\n\nnz_2 = 32\nSAMPLES_PER_CROSSING = 512\nh, h_diff = build_sinc_table(nz_2, SAMPLES_PER_CROSSING)\n\nout_imp_2 = sinc_resample(\n    impulse,\n    RESAMPLING_RATIO,\n    h,\n    h_diff,\n    SAMPLES_PER_CROSSING)\n\nfig = plt.figure(2)\nfig.set_figwidth(10)\nplt.subplot(211)\nplt.plot(out_imp_1)\n\nplt.subplot(212)\nplt.plot(out_imp_2)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nfrom scipy import fft\n\nNFFT = 1024\nimpulse_fft = fft.fft(out_imp_1, NFFT)\nimpulse_fft = fft.fftshift(impulse_fft)\n\nfft_db = 20 * np.log10(np.abs(impulse_fft))\nxf = fft.fftfreq(NFFT, 1/TARGET_FS)\nxf = fft.fftshift(xf)\nplt.figure(3)\nplt.plot(xf, fft_db)\nplt.xlim(0, TARGET_FS)\nplt.grid()\n\n# Ideal filter\nideal_x = np.zeros(22000)\nideal_x[-1] = -10\nplt.plot(ideal_x)",
    "crumbs": [
      "Posts",
      "Windowed Sinc",
      "Windowed sinc resampling"
    ]
  },
  {
    "objectID": "posts/reverb_fdn/index.html#feedback-delay-networks",
    "href": "posts/reverb_fdn/index.html#feedback-delay-networks",
    "title": "Automatic Optimization of Feedback Delay Networks",
    "section": "1.1 Feedback Delay Networks",
    "text": "1.1 Feedback Delay Networks\nFeedback Delay Networks (FDN) are a class of artificial reverberation algorithms that use several delay lines in a feedback loop in order to produce the reverberated sound [1].\nTypical components of a \\(N\\) channel FDN include the input gains \\(B\\), output gains \\(C\\), a feedback Matrix \\(A\\), \\(N\\) parallel delay lines, \\(N\\) attenuation Filters \\(H(z)\\) and a tonal correction Filter \\(T(z)\\).\n\n\n\n\n\n\nFigure 1: Common FDN structure [2]\n\n\n\nThe output of the FDN is given by the following equation, with \\(x(t)\\) being the input signal and \\(q_i(t)\\) the output of the \\(i^{th}\\) delay line [2]:\n\\[\ny(t) = \\sum_{i=1}^{N} c_i \\cdot q_i(t) + d \\cdot x(t)\n\\tag{1}\\]\n\\[\nq_j(t+m_j) = \\sum_{i=1}^{N} a_{ij} \\cdot q_i(t) + b_j \\cdot x(t) \\qquad (\\textrm{for}\\ 1 \\le j \\le N),\n\\tag{2}\\]\nwhere:\n\\[\nq(z) = \\begin{bmatrix} q_1(z) \\\\ q_2(z) \\\\ \\vdots \\\\ q_N(z) \\end{bmatrix} \\qquad\nD(z) = \\begin{bmatrix} z^{-m_1} & 0 & \\cdots & 0 \\\\ 0 & z^{-m_1} & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & z^{-m_n} \\end{bmatrix} \\qquad\nb = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_N \\end{bmatrix} \\qquad\nc = \\begin{bmatrix} c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_N \\end{bmatrix} \\qquad\nA = \\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1N} \\\\ a_{21} & a_{22} & \\cdots & a_{2N} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{N1} & a_{N2} & \\cdots & a_{NN} \\end{bmatrix}\n\\]\nThe transfer function of the FDN is then given by:\n\\[\nH(z) = c^T \\cdot \\left[ D(z^-1) - A \\right]^{-1} \\cdot b + d\n\\tag{3}\\]\nThe \\((\\cdot)^T\\) operator is the transpose operation, the matrix \\(D(z^{-1})\\) is the diagonal delay matrix, \\(A\\) is the feedback matrix, \\(b\\) is the input gain vector, \\(c\\) is the output gain vector and \\(d\\) is the direct path gain.\n\n1.1.1 Feedback Matrix\nThe feedback matrix, sometimes called the mixing matrix, is a \\(N\\times N\\) matrix that defines how the signal is scattered inside the feedback loop. Ignoring the attenuation filters for a moment, it is often desirable for the FDN to be lossless, which happens when the poles of the system all lie on the unit circle. This can be achieved with the use of a unitary feedback matrix [3], [4]. A matrix is said to be unitary if it fulfills the condition:\n\\[\nAA^H=I,\n\\]\nwhere \\((\\cdot)^H\\) denotes the Hermitian transpose and \\(I\\) is the identity matrix. Common types of unitary matrices include the Hadamard matrix and the Householder matrix. More complex types of matrix have been proposed in recent years with the goal of improving the echo density of the FDN output. Called filter feedback matrix (FFMs) [5], these are matrices where each entry is a finite impulse response (FIR) filter. These FFMs can be implemented efficiently in a cascaded form, where \\(K\\) feedback matrices are separated by a bank of delay lines, as shown in Figure 2.\n\n\n\n\n\n\nFigure 2: FFM structure from [5]\n\n\n\n\n\n1.1.2 Attenuation Filters\nAdding attenuation filters inside the feedback loop allows for control of the frequency-dependant decay time of the FDN. Various strategies have been proposed in the literature the design the filters. One of the first proposed methods involved the use of a one-pole lowpass filter[6] where the pole of the filter depends on the desired reverberation time (\\(T_{60}\\)) at \\(dc\\) (0Hz) and the Nyquist frequency and further scaled by the delay length of the preceding delay line. More recently, the use of graphic equalizers (GEQ) has been increasingly popular as low-order filters are not accurate enough to model the decay characteristics of real rooms [7]. Välimäki et al. [7] proposed a two-stage approach to design the attenuation filter where a low-order low-shelf filter is used to approximate the target magnitude response at the \\(dc\\) and Nyquist frequencies, followed by a higher-order GEQ that can be used to approximate the desired attenuation in multiple frequency bands. The use of the pre-filter eases the design of the GEQ which can become inaccurate if the gains variation between bands is greater than 12dB [7].\n\n\n\n\n\n\nFigure 3: Two-stage attenuation filter design from [7]\n\n\n\n\n\n\n\n\n\nFigure 4: Magnitude response of the two-stage attenuation filter design from [7]\n\n\n\nFor this project, an octave band GEQ composed of 10 cascaded biquad filters was used.\n\n\n1.1.3 Tonal Correction Filter\nA tonal correction filter is added to the output of the FDN to shape the initial energy of the impulse response. Jot [2] explains that the use of attenuation filters introduces a dependence between the reverberation time and the frequency response of the FDN. The tonal correction filter is then used to correct the frequency response of the FDN to match the desired target, which in our case would be a real room impulse response. A common method to obtain the gains of the filter is to look at the initial amplitude of each band of the energy decay curves.",
    "crumbs": [
      "Posts",
      "Automatic Optimization of Feedback Delay Networks"
    ]
  },
  {
    "objectID": "posts/reverb_fdn/index.html#results",
    "href": "posts/reverb_fdn/index.html#results",
    "title": "Automatic Optimization of Feedback Delay Networks",
    "section": "4.1 Results",
    "text": "4.1 Results\nThe MATLAB implementation of [9] (MatchReverb) was available online and was used to generate the following RIRs. The genetic optimization was performed over 5 generations with a population of 50 individuals per generation and took 23 minutes to optimze 3 FDN. The ‘Hybrid’ column was generated using the method described earlier with some modification, while the ‘FDN Only’ column uses the same method, but without the early reflection FIR filter. The target RIRs were taken from the MIT RIR survey[19] and can be found online. Three rooms were chosen for the analysis-synthesis: h001_bedroom, h025_diningroom, and h042_hallway, with average reverberation times of 0.30, 0.94 and 2.02 seconds respectively[17]. The optimization time for the three rooms was around 10 minutes. Some of the modifications found in the source code, which were not in the original paper, include a modification to the fitness function that now takes into account the mean error between the target RIR EDC and the synthesized ones, as well as a bug in the scripts that causes the output gains of the FDN to be set to 1. Another interesting aspect of the implementation is that, while the paper claims the tone correction filter is designed in the analysis stage (ie. before the optimization), the implementation actually first generates an impulse response without any tone filter and then the filter is designed using the energy difference between the initial spectrum of the optimized FDN and the initial spectrum of the target RIR. Furthermore, the paper claims to use the MFCCs of the impulse responses to compute the fitness function, but the implementation actually uses the mel spectrogram of the impulse responses.\nWhile the results of the hybrid FDN sound quite close to the reference RIR, the poor results of the FDN-only version might suggest that the early reflection FIR is doing most of the heavy lifting. The ‘Replica’ column was generated using my own reimplementation of the methodology. My version of the genetic algorithm was performed over 40 generations with a population of 50 individual and took 11 minutes to optimize the 3 FDNs. The higher generation number did not translate into much better results, as it was found that the FDNs tended to converge towards a local minimum in the first 10 generations.\n\n4.1.1 Impulse responses\n\n\n\n\n\n\n\n\n\n\nRIR\nReference\nMatchReverb - Hybrid\nMatchReverb - FDN Only\nReplica\n\n\n\n\nBedroom\nYour browser does not support the audio tag.\nYour browser does not support the audio tag.\nYour browser does not support the audio tag.\nYour browser does not support the audio tag.\n\n\nDining Room\nYour browser does not support the audio tag.\nYour browser does not support the audio tag.\nYour browser does not support the audio tag.\nYour browser does not support the audio tag.\n\n\nHallway\nYour browser does not support the audio tag.\nYour browser does not support the audio tag.\nYour browser does not support the audio tag.\nYour browser does not support the audio tag.\n\n\n\n\n\n4.1.2 Audio Examples - Percussion\n\n\n\n\n\n\n\n\n\n\nRIR\nReference\nMatchReverb - Hybrid\nMatchReverb - FDN Only\nReplica\n\n\n\n\nBedroom\nYour browser does not support the audio tag.\nYour browser does not support the audio tag.\nYour browser does not support the audio tag.\nYour browser does not support the audio tag.\n\n\nDining Room\nYour browser does not support the audio tag.\nYour browser does not support the audio tag.\nYour browser does not support the audio tag.\nYour browser does not support the audio tag.\n\n\nHallway\nYour browser does not support the audio tag.\nYour browser does not support the audio tag.\nYour browser does not support the audio tag.\nYour browser does not support the audio tag.\n\n\n\n\n\n4.1.3 Audio Examples - Drums\n\n\n\n\n\n\n\n\n\n\nRIR\nReference\nMatchReverb - Hybrid\nMatchReverb - FDN Only\nReplica\n\n\n\n\nBedroom\nYour browser does not support the audio tag.\nYour browser does not support the audio tag.\nYour browser does not support the audio tag.\nYour browser does not support the audio tag.\n\n\nDining Room\nYour browser does not support the audio tag.\nYour browser does not support the audio tag.\nYour browser does not support the audio tag.\nYour browser does not support the audio tag.\n\n\nHallway\nYour browser does not support the audio tag.\nYour browser does not support the audio tag.\nYour browser does not support the audio tag.\nYour browser does not support the audio tag.",
    "crumbs": [
      "Posts",
      "Automatic Optimization of Feedback Delay Networks"
    ]
  },
  {
    "objectID": "posts/reverb_fdn/index.html#error-visualization",
    "href": "posts/reverb_fdn/index.html#error-visualization",
    "title": "Automatic Optimization of Feedback Delay Networks",
    "section": "4.2 Error visualization",
    "text": "4.2 Error visualization\nFigure 8 shows the mel spectrogram of the target RIR (right), the FDN output (middle), and the error between the two (left). The error is the absolute difference between the two spectrograms, with a mean error of ~2dB. Even though the error between the two spectrograms is small, the FDN output still sounds noticeably different from the target RIR. This suggests that a fitness function solely based on the spectrogram is not sufficient to accurately match the target RIR using GA.\n\n\n\n\n\n\nFigure 8: Mel spectrogram of the target RIR (right), the FDN output (middle), and the error between the two (left). The error is the absolute difference between the two spectrograms.",
    "crumbs": [
      "Posts",
      "Automatic Optimization of Feedback Delay Networks"
    ]
  },
  {
    "objectID": "posts/reverb_fdn/index.html#result",
    "href": "posts/reverb_fdn/index.html#result",
    "title": "Automatic Optimization of Feedback Delay Networks",
    "section": "5.1 Result",
    "text": "5.1 Result\n\n5.1.1 Impulse responses\n\n\n\n\n\n\n\n\nRIR\nReference\nFDN\n\n\n\n\nBedroom\nYour browser does not support the audio tag.\nYour browser does not support the audio tag.\n\n\nDining Room\nYour browser does not support the audio tag.\nYour browser does not support the audio tag.\n\n\nHallway\nYour browser does not support the audio tag.\nYour browser does not support the audio tag.\n\n\n\n\n\n5.1.2 Audio Examples - Percussion\n\n\n\n\n\n\n\n\n\nRIR\nReference\nFDN - Scattering\nFDN - Householder\n\n\n\n\nBedroom\nYour browser does not support the audio tag.\nYour browser does not support the audio tag.\nYour browser does not support the audio tag.\n\n\nDining Room\nYour browser does not support the audio tag.\nYour browser does not support the audio tag.\nYour browser does not support the audio tag.\n\n\nHallway\nYour browser does not support the audio tag.\nYour browser does not support the audio tag.\nYour browser does not support the audio tag.\n\n\n\n\n\n5.1.3 Audio Examples - Drums\n\n\n\n\n\n\n\n\n\nRIR\nReference\nFDN - Scattering\nFDN - Householder\n\n\n\n\nBedroom\nYour browser does not support the audio tag.\nYour browser does not support the audio tag.\nYour browser does not support the audio tag.\n\n\nDining Room\nYour browser does not support the audio tag.\nYour browser does not support the audio tag.\nYour browser does not support the audio tag.\n\n\nHallway\nYour browser does not support the audio tag.\nYour browser does not support the audio tag.\nYour browser does not support the audio tag.",
    "crumbs": [
      "Posts",
      "Automatic Optimization of Feedback Delay Networks"
    ]
  },
  {
    "objectID": "posts/reverb_fdn/index.html#error-visualization-1",
    "href": "posts/reverb_fdn/index.html#error-visualization-1",
    "title": "Automatic Optimization of Feedback Delay Networks",
    "section": "5.2 Error visualization",
    "text": "5.2 Error visualization\nFigure 9 shows the magnitude response of the FDN before and after optimization. The optimized FDN shows a magnitude response with a greater number of peaks and a lower standard deviation. Another useful visualization can be achieved by plotting only the peaks of the full magnitude response. It shows that the distribution of the peaks is much narrower after optimization, which should result in less coloration in the resulting impulse response [23].\n\n\n\n\n\n\n\n\nMagnitude response of a colorless FDN before optimization (top) and after optimization (bottom).\n\n\n\n\n\n\n\nMagnitude response before (top) and after (bottom) optimization. Only the peaks of the response are plotted\n\n\n\n\n\n\nFigure 9",
    "crumbs": [
      "Posts",
      "Automatic Optimization of Feedback Delay Networks"
    ]
  },
  {
    "objectID": "posts/reverb_fdn/index.html#spectrograms-of-synthesized-impulse-responses",
    "href": "posts/reverb_fdn/index.html#spectrograms-of-synthesized-impulse-responses",
    "title": "Automatic Optimization of Feedback Delay Networks",
    "section": "7.1 Spectrograms of synthesized impulse responses",
    "text": "7.1 Spectrograms of synthesized impulse responses\nImpulse responses and spectrograms of the target RIR, the synthesized RIR using GA and the synthesized RIR using RIR2FDN. The yellow line indicates the echo density profile as proposed by [25]. The red line indicates the time at which the echo density profile reaches a value of 1.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 10:",
    "crumbs": [
      "Posts",
      "Automatic Optimization of Feedback Delay Networks"
    ]
  },
  {
    "objectID": "posts/reverb_fdn/index.html#fdn-performance",
    "href": "posts/reverb_fdn/index.html#fdn-performance",
    "title": "Automatic Optimization of Feedback Delay Networks",
    "section": "7.2 FDN performance",
    "text": "7.2 FDN performance\nBenchmarks of various FDN configurations. The only difference between the configurations is the number of cascaded biquads used in the attenuation and tone correction filters. A Householder matrix was used as the feedback matrix.\n\nBlue: 11 cascaded biquads (i.e. octave band filters)\nOrange: 32 cascaded biquads (i.e. 1/3-octave band filters)\nGreen: 1 biquad\n\nThe benchmarks were performed on a 2024 MacBook Air M3 using the microbenchmarking library nanobench and measured the time taken to render 1 second of audio at 48kHz.",
    "crumbs": [
      "Posts",
      "Automatic Optimization of Feedback Delay Networks"
    ]
  },
  {
    "objectID": "posts/bowed_string/bowed_string_example.html",
    "href": "posts/bowed_string/bowed_string_example.html",
    "title": "BowedString Part 3: BowedString audio example",
    "section": "",
    "text": "Here are a few audio examples of the BowedString instrument in its current state. As a reminder, what we have right now is a digital waveguide with a nut and bridge. The bridge has a slight low-pass filter. The string is excited by the bow model we discussed in part 2.\nIf these sound cold and robotic, that is to be expected. The body filter is missing and all of these samples were generated by C++ code with all of the parameters moving in a perfectly linear way. It is helpful to have simple and easily reproducible sounds to test the code with and will make debugging and tuning the instrument much easier.\n\nVelocity and Force sweep from 0 to 1 then back to 0.\n\n\n\n\n\n\n\n\nFast bowing by modulating velocity with a clipped sine wave. A staccato sound can be achieved by reducing the amount of clipping.\n\n\n\n\n\n\n\n\nVibrato. The length of the string is modulated by a sine wave.\n\n\n\n\n\n\n\n\nPitch slide, 440Hz to 660Hz.",
    "crumbs": [
      "Posts",
      "Bowed String",
      "BowedString Part 3: BowedString audio example"
    ]
  },
  {
    "objectID": "posts/phaseshaper/phaseshaper.html",
    "href": "posts/phaseshaper/phaseshaper.html",
    "title": "Phaseshaping Oscillator",
    "section": "",
    "text": "Phaseshaper is a eurorack oscillator built on the Daisy Patch. The module implements 7 phaseshaping techniques as presented in Phaseshaping Oscillator Algorithms For Musical Sound Synthesis by Jari Kleimola, Victor Lazzarini, Joseph Timoney, and Vesa Välimäki. The C++ implementation of the phaseshaping algorithm was largely based on the original code in Python. Each phaseshaping technique can also be modulated, as described in the paper. The module also allows the possibility to crossfade between each waveform, providing an even wider range of timbres.\nHere you can find captures of the different waveforms and how they respond to modulation.\n\n\n\n\nHardsync\n\n\n\n\n\nSupersaw\n\n\n\n\n\nSoftsync\n\n\n\n\n\nWaveslice\n\n\n\n\n\n\nVariable-slope: Ramp Phase\n\n\n\n\n\nVariable-slope: Triangular Phase\n\n\n\n\n\nTriangle Modulation\n\n\n\n\n\nAnd here’s some audio samples of the different waveforms:\n\n\nSupersaw\n\n\n\n\n\n\n\n\nVariable-slope, Triangular phase\n\n\n\n\n\n\n\n\nVariable-slope, Ramp phase\n\n\n\n\n\n\n\n\nWaveslice\n\n\n\n\n\n\n\n\nSoftsync",
    "crumbs": [
      "Posts",
      "Phaseshaper",
      "Phaseshaping Oscillator"
    ]
  }
]